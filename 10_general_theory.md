# General Theory of Animistic Agency

## Introduction

Existence is intimately tied to perception, as famously made clear by Descartes when he said "I
think therefore I am". Indeed at a very deep metaphysical level the existence of reality is
contingent on the property of self-perception, as there is no frame of reference which can be seen
as 'outside' the universe. As thinking beings embodied through the reality of this universe, we have
a caretaker's responsibility to participate in creation through our perception and agency.

How should we understand agency? What does it mean to act? Can the finite and infinite co-exist?
This General Theory of Animistic Agency is our attempt to derive a more productive framework for
grappling with these sorts of questions than we have been able to find.

Whereas most [metaphysics][] attempts to build formal ontologies that are true outside the
contingencies of being, we start by presupposing that no ontology is unconditionally true. Instead,
within all possible ontologies, there exist ontologies more and less suited to achieving particular
ends within particular environments.

As stated in the [objective][], our metaphysics must help us orient towards what is good and
meaningful. This theory helps us identify the good and meaningful by providing useful derivations of
what we metaphysically mean with respect to "orientation", "good", and "meaningful".

We hope this metaphysical framework is suitable for general application, yet we acknowledge its
fundamental incompleteness. Our aims are grand; we have reason to believe Animistic Agency has a
role to play in many places across society. Agency permeates the phenomenology of existence, this is
the truth of Animistic Agency. Yet, existence as we know it is constrained by our milieu. We are
human. We write these words in the year 2023. We live in America. How much of our truth transfers to
the whole breadth of society that needs healing remains to be seen. We acknowledge the following
noteworthy assumptions within the theory.

We are concerned with developing an ontological framework for beings who's ability to sense and act
is limited by their embodied perspective and potentials. The amount of information they can process
and act upon is always finite and localized. All being may be described as embodied, but does our
phenomenological *sense* of embodiment overly influence our conclusions?

We are hominid apes, who's being is intimately tied to our evolutionary history. The social role of
communication and coordination factors heavily into our argument. Are these phenomena necessary for
being, or simply contingent on our physical and cultural evolution? We do not know, but their role
remains central within Animistic Agency.

Whereas most inference assumes the Uniformity Principle (@induction-problem), many of our core
arguments assume the exact opposite, what we will call the [Action Postulate][], that the only
constant in greater reality is change. How else can we explain the energy that moves us and makes
time proceed? Here we place action as the core mystery of the universe, so here we presume that
underlying all the order we find within our reality is a common substrate with infinite capacity for
action. This is discussed further in the following section [Natural and Normative Agency][].


## Agency and Intention

The term 'agency' is well understood colloquially, but there is a lot of wiggle room in its usage,
and we intend to use the term precisely, definitions.

[Agency]{#agency}

:   An entity displays agency when its properties cannot be fully explained as effects of
    environmental phenomena external to the entity.

Agency is the essence of action embedded within an entity. When we presume maximal agency onto an
entity we open up the realm of full potential with the universe by the space carved out by the
entity. By presuming complete agency, we infer that the unobserved structure of that entity not only
contains the capacity to do *anything* that is not otherwise restricted by the embedding
environment, but also that we have no insight into what the probable actions are. So, unless we can
observe structural characteristics within the entity, when making inferences on the entity we adopt
the [Action Postulate][].

[AP]: #action-postulate
[Action Postulate]: #action-postulate
[Action Postulate]{#action-postulate}

:   All possible next states of a system are equally probable unless the system is internally
    structured to sort possible states into preferred states.

For most entities we do observe preferences in their actions. Even if we cannot observe their
complete inner workings, we can infer probable next action based on how we have observed them to act
in the past. The entity displays preferences on its next action that influences what it may possibly
do and orders it into what it will probably do.

[Intention]{#intention}

:   An ordering of available *potential* actions from most to least favored, such that the most
    favored are the most *probable* to be acted upon.

By inferring intention onto an entity we acknowledge we can not observe it fully and/or do not
understand it completely. Note that this statement is equally valid for understanding our internal
phenomenological experience as well as understanding our environmental observations. We experience
phenomenological intention without a full epistemic understanding of the mechanism of intention,
rather it is an emergent way to understand and influence our actions and preferences. We can reframe
intention as a discrete observation of normative action. We take the property of normativity to be
defined as thus:

[Normativity]{#normative-order}

:   The capacity to identify possible actions and preferentially accommodate some possible actions
    over others.

For an entity to possess normativity it must act preferentially. It must exhibit consistency in
intention even when its internal and external environment deviate.

[Agent]{#agent}

:   An entity that exhibits consistent intention in its agency through embodiment of a normative
    structure.

See [Figure 1](#embedded-agency) for a cartoon representation of agency, bounded predictive
certainty, as well as ancillary supporting terminology.

![We observe acts of agency at an *agential boundary*. This boundary separates an entity out of the
observable environment. The entity's interior is an *agent*. By inferring agency within the
observable environment we can identify a source of uncertainty on the evolution of the
environment.](figures/embedded_agency_1.png){#embedded-agency width=75%}

We suppose agency on an entity in order to hypothesize why the entity behaves the way we observe it
to. If we can increase our predictive capacity by introducing agency into our representation of an
environment, then agency is a useful property to label the entity with. By labeling an entity as
having agency, we identify it as a source of environmental variability that we chose not to
ignore. By identifying sources of agency we can increase our predictive capacity for that
environment by constraining our beliefs on the environment's probable changes. The more in-tune our
predictions are with the actual evolution of the environment, the greater our environmental fitness
is, as we are better able to exploit opportunities and avoid catastrophes.

Yet if we assume absolute agency on an entity, we must also assume it has no intentions and we are
therefore maximally uncertain about its next action. If we don't know what guides and constrains an
entity's internal drive, then we must adopt the safest option, which is to afford equal weight to
all available actions. This position is unappealing as it dramatically constrains our confidence in
how the environment is likely to evolve.

What then are we to do? Through careful observation we can readily infer the functional elements of
the entity; here are eyes, here are arms, here a mouth, and so on. These properties limit the
entity's possible next action, but they do not tell us anything about what compels the entity to use
those facilities in one way versus another. This knowledge is unavailable to us, so why don't we
adopt a defensive posture and always prepare ourselves to expect the unexpected from one another?
Generally we don't, and instead adopt Hume's Uniformity Principle, and infer that future events are
likely to resemble what we have experienced in the past.

What basis do we have for such inference? In general we cannot deduce what the future will look
like, we can only induce future events. To date, there is no commonly agreed upon epistemic
foundation to the process of induction. We cannot say more than we believe the future will generally
look similar to what we have so far experienced (@induction-problem).

We believe this to be a misframing of the true nature of inference. If we are to infer one event is
more likely to occur over another we must both believe that action will continue, and believe that
action will be directed by something that exists right now. The basis of these beliefs must come
from our subjective experience, not from some external reality. We experience states of intention
and more often than not we act on those intentions. A thread of experience supports this
self-truth. We do not need to infer our own actions from external observable phenomena, we can
deduce action from our self-observation of intention. The fundamental inferential jump is that the
observable environment operates similarly to our subjective experience. Subjectively, to act is to
use our agency. To *direct* action is to act intentionally.

Agency is what compels us to act on what is possible. Agency does not care about what action we end
up taking, so long as action continues. Intention is the subjective experience of how we guide this
motive force. When we infer agency exists outside ourselves, we infer such an intention-driven
compulsion constrains the entity's *possible* next actions into a more limited set of *probable*
actions. We can only formulate what is possible and probable based on our own knowledge of intention
and agency, therefore to infer the probable onto the observed environment is to place a piece of our
subjectivity into the perspective of that entity and infer its intentions based on our own capacity
for intention.

![Inferring intention to agency increases our predictive
certainty.](figures/embedded_agency_2.png){#embedded-agency-2 width=75%}


[Theorem 1]: #theorem-1
[Theorem 1: Causal Inference Implies Attribution of Agency]: #theorem-1
[Theorem 1: Causal Inference Implies Attribution of Agency]{#theorem-1}

:   To make a causal inference which constrains a set of *possible* next states into a more limited
    set of *probable* next states, one inherently presupposes intentional agency exists within the
    system. Intentional agency is a localized coupling between action preferences (intentions) and
    possible actions.

Therefore, by simply modeling an external entity in order to predict its evolution, we inherently
treat the entity as having agency within the environment. Our models do not even need to accurately
map to the interior of an entity for them to be effective. For example, if we see a stream, we can
say it *wants* to flow to the sea, and gain some real predictive power from that inference even
though the underlying causal force does not reside in the stream at all, nor does the stream contain
any real internal state, let alone the subjective desire to meet the sea. We do not know how exactly
the stream will behave at any particular day or at any particular bend in its bed, but we have
constrained our uncertainty. We know it won't arbitrarily reverse course for example. Of course we
can also align agential boundaries with epistemological knowledge as well. It is accurate to say
that two masses are gravitationally *attracted* to one another, and this is still agential language.

By trying to predict an entity's probable next action, we form an intimate relationship with it. Not
only does this act potentially improve our own fitness, we essentially co-create a derivative agency
within our subjective universe whenever form such a prediction. The modeled entity is essentially a
child of our subjective ontology mixed with what the universe provided as input stimuli that
triggered us to identify and delineate the entity from the rest of our arena in the first
place. When we predict what the environmental entity will do by referring to our modeled instance,
what is the agency of the model itself? It must exist in the [imaginal][] realm. It is not part of
our agency, yet it is also not in the same realm we are embedded within. Instead it lives, but in a
realm that exists between our subjectivity and that of the greater objective universe.

Our perceptions of agency and intention are therefore just as important as the entity's experience
itself. For, as subjective creatures all we can say with certainty is we think therefore we are. Our
individual subjectivities are the only axiomatic truths each of us can be confident of. We can infer
external agency through sensory perception, but such an objective entity can never be experienced
within our individual psyches directly. Instead it is a product of perception as filtered through
subjective ontology. A myriad of layers of modeled phenomena stand between subjective experiences of
external agency and any objectively true external agent.


## A Taxonomy of Agency

[Complex Agent]: #complexity-and-capacity
[Capable Agent]: #complexity-and-capacity
[Agential Complexity]: #complexity-and-capacity
[Agential Capacity]: #complexity-and-capacity
### Complexity and Capacity

Agency exists in a landscape of *complexity* and *capacity*, where more complex agents contain the
potential to cause more complicated effects and more capable agents contain the potential to cause
more forceful effects. Agential complexity is an attribute of the depth of an agent's normative
order, and agential capacity is an attribute of the characteristics of the agent's boundaries
(including its breadth and proximity to the [Natural Substrate][]. The following example helps to
demonstrate the impacts of complexity and capacity in agential action. A stream (shallow normative
order) flows within its banks, unless a beaver (deep normative order) dams the stream, causing the
stream to overflow its banks and creating a pond. More capable entities contain the potential to
cause more forceful effects --- a rainstorm (large agential boundaries generate large potentials)
overwhelms the dam (small boundaries), reverting the stream to its original boundaries. Complex
agents with large boundaries contain the potential to cause forceful and complicated effects --- a
human city (deep order, large boundaries) changes the forest through (mis)management and thereby
shifts the weather patterns, changes the beaver's number and behavior, and changes the streams route
and might.

[Natural Agent]: #natural-and-normative-agency
[Normative Agency]: #natural-and-normative-agency
### Natural and Normative Agency

We classify possible agential entities onto a spectrum based on the characteristics of their
boundaries and embodied intentions. This spectrum corresponds to the real number line. It maps from
the infinite to the infinitesimal. The boundless expanse of the infinite boundary we label *natural
agency* and the impossible precision of the infinitesimal boundary we label *normative
agency*. Where an entity resides on this spectrum determines its capacity to hold and transform
value. The Natural Agent embodies pure [agency][] within infinite boundaries. All possible
structures of discrete agency are contained within its boundaries. The Natural agent accommodates
every imaginable pattern and has no preferences between them. It simultaneously exhibits maximal
agential complexity and capacity while possessing no normativity. A purely normative agent
accommodates a single bit of pattern and has a single preference. Such an agent has infinitesimal
agential capacity and minimal complexity.

[Natural Substrate Postulate]: #substrate-posulate
[Substrate Postulate]: #substrate-posulate
[Natural Substrate]: #substrate-posulate
[Natural Substrate Postulate]{#substrate-postulate}

:   We view the ultimate substrate of existence as unknowable, and therefore attribute ultimate
    agency to it. We call this substrate the Natural Agent. For ultimate agency to co-exist with
    subjective awareness, it must accommodate discrete structures of ordered action. For the
    infinite and finite to co-exist, we postulate that the Natural Agent has the following
    properties. Through these properties, the natural agent defines the possible structure and
    dynamics of reality:

    1. It is infinitely accommodating to all possible normative structures, and
    2. It imbues normative structures with agency, thereby granting bounded entities the capacity to
       modify their state based on their current internal and adjacent states.

    The nature of observed reality is thus the evolution of an initial ordering of discrete
    structures. The Natural substrate accommodates all possible initial sets of rules and entities,
    and computes their progression infinitely.

Through its boundless nature, the natural agent is infinitely accommodating. To be boundless requires
non-exclusionary behavior, otherwise boundaries manifest. Every permutation of matter, energy, and
information can occur and every possible structure must be accommodated. Accommodation in this sense
means complete freedom to act.

In opposition, pure normative agency is perfectly deterministic. The order of pure normative agency
is a single bit. The state of its infinitesimal boundary is either changed or not. Change at this
limit is discrete. How it manipulates the physical space along its border is a completely
deterministic property of the border's current state and the system's normative order. Given
observable access to the state of a normative agent, pure normative agency is perfectly
predictable. By its nature, the natural agent contains all *normativity*, and pure normative agency
contains a single self-consistent slice of normative space.

Within the manifold of the natural substrate, every variant of normative agency must be initially
accommodated, yet only a subset of those normative systems are stable. To be stable, a normative
agent must dynamically reinforce its embodied state. It must exhibit the property of [Autopoiesis][]
(auto: from self, poiesis: to bring into being that which did not exist before). Autopoiesis refers
to the property of certain entities to self preserve and self propagate. This term was coined to
identify the core properties that differentiate living systems from the rest of the natural
world. It is our belief that these properties transcend individual organisms and describe the
structure of many other phenomena that co-inhabit our reality.

A consequence of the [Action Postulate][] and [Substrate Postulate][] is that all observed *order*
is a consequence of local structure. If the local structure changes, then the nature of the order
changes as well. Autopoiesis is the property of certain structures to resist this decay of their
intrinsic order. Given our definitions, each normative system has a finite boundary and is embedded
within the infinitely accommodating expanse of the Natural Substrate. Therefore each normative agent
must accommodate adjacent systems who are likewise naturally embedded. When neighboring systems
conflict (as any two neighboring normative entities will), then their states will change as the
agents interact. An entity without autopoiesis is therefore unstable, as it cannot self regulate its
values or its boundaries in light of inevitable change. Because it is unstable, we cannot attribute
any intention or boundary to the entity. Its state is best represented as constituent of a different
level of organization. Within a world governed by the action and substrate postulates, the only
phenomena we can possibly reference must be a consequence of autopoietic organization. Without
autopoiesis, the referent no longer exists once the reference is formed.

[Theorem 2]: #autopoietic-reference
Theorem 2: All Stable Phenomena Are Causally Attributable to Autopoietic Entities

:   Any recurring phenomenon is causally attributable to the interaction between one or more
    autopoietic entities.

Within the Natural Substrate, we group all observable phenomena as emerging from autopoietic
agency. This includes the natural laws of the observable universe. We will refer to "natural agents"
as the set of entities close to the natural limit of the agency spectrum that exist close to the
limits of the observable universe. Natural agents define the natural forces of the universe, so
their boundaries are (near) limitless as their substance stretches across creation. Because of the
breadth of their boundaries, their capacity for agency is limited but existent.

Natural agents are the base layer of stable sorts on top of the manifold of the natural
substrate. All observable natural forces are intrinsically stable acts of sorting, as they push and
pull matter and energy according to contextual cues. Where does the stability of these forces come
from? We are not prepared to answer that question, that is the foundational question of
physics. Sill, we can infer that structure comes from one of two relationships: either each
fundamental force is due to profoundly normative autopoietic entities that have colonized the
observable universe, or reality intrinsically and continuously spawns the phenomena resulting in the
fundamental forces due to some unobserved aspect of its fundamental structure. Inherently these
choices boil down to the same conclusion. The observable universe is autopoietic. In the former case
this conclusion is clear. In the latter case, the fact that the base properties of reality are
stable speaks to its capacity to sort and therefore hold consistent values.

Epistemically our assertion that the natural world is fundamentally agential is unknowable. Yet for
practical reasons this inference makes sense. First, if agency is the source of action, then it
matches our internal subjective experience of continued action. Secondly, if we assume the natural
world is agential and therefore possesses normativity intrinsic to its structure, we acknowledge our
limits. Something agential may experience some type of subjectivity. Subjectivity is intrinsically
opaque to a viewer who is not within the cognitive structure producing the subjective
experience. Whenever we assume subjectivity onto a phenomena, we acknowledge its capacity to change
and surprise us. When we hold this viewpoint we are more prepared to be surprised, which is a
helpful position to have if one wants to avoid catastrophe.


## The Cognitive Limits of Embodied Being

What does it mean to be embodied? To be an embodied being is to be embedded within a reality that
transcends one's embodiment. It is to exist within known as well as unknown limits. For a being to
be embodied implies a cognitive process such that the phenomenological states of being, attention,
and intention are possible. We treat a cognitive process as a special subset of information
processes, it is the type of information process that is necessary but not wholly sufficient to
produce the phenomenon of being. We must start our discourse by defining the limits to such a
cognitive process. These limits prescribe the first layer of limits on the space of possible
ontologies that may fulfill our [objective][].

To *be* embedded in reality implies limits. To *continue to be* implies self-regulation. Reality is
ever changing, such that all phenomena are ephemeral. To maintain a thread of being within the
ever-changing landscape of reality requires resisting change when that change could disrupt the
structures by which one's being is instantiated.

To *be* doe not imply self-regulation, but *continuing to be* does. Autopoiesis does not imply
phenomenological self-awareness, but self-awareness does imply autopoiesis. For any self---whether
self-aware or not---to *desire* continued existence implies the self is autopoietic.

What then are the constituent properties of (successful) autopoiesis?

1. Preservation of boundaries between self and environment.
1. Capacity to sense and respond to the environment in order to protect the self-structure from
   adverse changes as well as profit from windfalls within the environment.
1. Ability to process information such that terms like sense and respond have self-meaning.
1. Ability to process information such that self-action occurs frequently enough to maintain the
   self-structure within the environment.
1. Ability to discern adverse from advantageous states within the environment.
1. Ability to harness energy so as to maintain the capacity to sense and act.
1. Ability to maintain and/or reproduce the self-structure.

Many of these properties are context sensitive, but the first five stand out in their computational
generalizability. To be autopoietic entails having a structure capable of processing information
according to the constraints of existing within a reality incomprehensibly larger and more complex
than the computational self. Each self-structure exists within a different network of constraints,
and yet they all must process information in response to the dynamics of the environment they are
embedded within. Senses and actions must be on time and sufficient to respond to the vagaries of the
current state of the self and its environment.

Here we run into an information-theoretic constraint that limits the possible design of any
self-actualizing system, this constraint is called the [Frame Problem][]. Please refer to the
glossary for a comprehensive definition, but in short the [Frame Problem][] identifies a fundamental
issue associated with tokenizing an unlabeled infinite bandwidth stream of input data into a set of
inferred facts and actions that are *relevant* to the self's continued existence. The issue breaks
into two sub-problems: 1) how to define such a procedure without presupposing what is relevant
(i.e. falling into the [homuncular fallacy][]), and 2) how the procedure can continuously update
itself such that relevance remains context-sensitive---tuned to the moment of action---and not to
any static definition of relevance.

The theory of [Relevance Realization][] offers a compelling thesis for how these issues might be
addressed. Relevance Realization suggests that the frame problem can be surmounted by the dynamics
of [opponent processing][]. Within a relevance realizing system multiple independent automata work
for opposing goals within a shared information substrate. Given suitable automata, the substrate
will exhibit three emergent properties, [Cognitive Scope][] (CS), [Cognitive Tempering][] (CT), and
[Cognitive Prioritization][] (CP). When a substrate exhibits these properties, then it can be
tokenized into a tape of instructions for each constituent automata that is suitable for the
collective to act on what is relevant with respect to their shared embodied condition. One way of
explaining Relevance Realization is that a system that realizes relevance exhibits a healthy
cognitive ecology. No single automaton realizes relevance for the system, yet through their mutual
interaction, the aggregate system exhibits the properties of CS, CT, and CP; thereby differentiating
and translating the cascade of input phenomena (autopoietically successful) actions by the system.


## A Formal Language of Autopoietic Agency

TODO

:   In this section we identify the core concepts necessary to formally model the interactions
    between multi-layered autopoietic entities.

    After defining these concepts, we show how they can be combined to dynamically simulate the
    interaction between such entities.

    Having such a formalization enables us to explore the interaction between incentives across a
    pluralism of agencies that ranges from the natural agent to any possible normative orders built
    up from that base.

Indeed the paradigm must be malleable enough to fit into each of our subjective frameworks yet
structured enough that we can coordinate across the entire realm of predictive models across society
at large.

In this section we lay out core abstractions for a formal language to discuss such matters. We deem
these concepts to be necessary, but not sufficient for defining a framework to coordinate with one
another on such questions.

Each definition proceeds as a rationalization for why we need the concept, followed by a formal
definition of the required term. The section culminates in a grammar of agency by which we can
discuss the characteristics of, as well as model the interactions between, various sets of
autopoietic agential entities.


### Referent and Reference

In prior sections ([Introduction][] and [Agency and Intention][]) we hoisted certain phenomena of
subjective experience as primary within our native ontology. We do this not necessarily because of
any assertion of about the true nature of reality *is*, but rather because that is the only means
we, as subjective embodied beings, can ground what we observe in unassailable postulates of
truth. This framework presents interesting issues related to reference and relationship if we desire
to coordinate our observations and knowledge precisely and rigorously. If one's core truths are only
necessarily axiomatic intrinsically, and one's ontology is built from those truths, how do you share
with another being who you can only infer has a subjective experience? Worse yet, when the ground
truth underpinning our intention and agency is subjectively derived, how is one to coordinate with
another to agree on what is true, meaningful, or good?

This section addresses the intricacy of referring to an entity within an environment when neither
the entity nor the environment is known or understood in any base sense. Instead all quantities are
references to observations, and whose behavior is inferred but cannot be known or deduced in any
absolute sense.

1. To refer to an entity, we need properties by which to separate that entity from all that is
   possible. If we presume that sensory information can be subjectively processed as a stream
   of discrete information within some frame, we can define properties as follows.

   [Property]: #property
   [Property]{#property}

   :   A pattern within a series of symbols that can be identified through a [regular language][]. A
       property is a tuple consisting of the regular expression and the captured pattern.

   We infer that the properties we identify are also identifiable using alternative languages and
   symbols, and therefore translatable across multiple ontological frames of reference.

1. To define an entity, we must be able to dynamically reference it within an embedding space.

   1. To create such a reference, there must be a boundary condition that delineates the entity with
      respect to the substrate. Notably, boundaries do not need to be closed, static, or atomic in
      any sense. Indeed, there may be no such thing as an entirely "closed" boundary, as every
      physical or logical phenomena can be disrupted. Boundaries are simply local differential
      conditions we can continue to identify even as the information substrate changes.

   1. To have a boundary entails slicing an information substrate into three regions: we'll call
      these the inside, outside, and manifold. The inside and outside are the same dimension as the
      substrate, while the manifold is one dimension smaller.

   1. If the manifold is closed, then the inside and outside are separated, otherwise they are one
      and the same. When the manifold is closed, we define the entity to consist of the enclosing
      manifold as well as the enclosed substrate region.

   1. If the manifold is closed, the entity is of the same dimension as the substrate. If it is
      open, the entity is a dimension smaller.

   1. In either case, the entity is a localized phenomenon of the substrate, it is constituted by
      the properties of the substrate.

   1. A boundary definition must be dynamic in reference to its embedding space if we want to use it
      to identify and infer agency. The patterns which identify the manifold must be recognizable
      even as the substrate dynamically evolves. Without such a relative description any model we
      form of the entity's intentions will quickly lose all explanatory power. If we cannot locate
      the entity, we cannot infer its intentions.

1. Furthermore, to refer to an entity, we must also suppose a substrate within which the entity
   exists. We refer to this substrate as the entity's information substrate. Out of all the possible
   properties, it must encompass at least all the properties necessary to dynamically reference the
   entity and its actions.

   1. An information substrate must accommodate the concept of localization, where the nature of the
      properties can vary across the field of the substrate.

   1. To refer to properties that are localized within the substrate requires a frame of reference
      and a unit of measure to describe where the localized properties exist within the substrate.

   1. To define a frame of reference and unit of measure entails defining a reference location and
      orientation within the information substrate. Therefore we must a priori adopt a referent
      entity within the substrate to relate the local properties of any other reference entity to.

   1. We must infer the referent entity intends to maintain internal consistency such that it can
      function as a consistently observable referent during our observation.

   1. We can then identify and refer to an entity as a certain pattern of properties within the
      substrate with respect to its relationship with the referent.

   [Information Substrate]: #information-substrate
   [Information Substrate]{#information-substrate}

   :   A field of properties within which we can characterize dynamic entities. An information
       substrate representation is emergent and super-dimensional. Such representations are valid
       wherever a semi-stable information medium is available. A substrate requires a reference
       frame, unit of measure, and entities defined with respect to the reference frame that consist
       of semi-stable properties. The substrate may be either derived from a natural order, or a
       shared normative order.


   1. In order to maintain a referent by which we have communicable access to an information
      substrate, the properties of that referent must maintain their consistency. To hold properties
      constant entails it is highly probable the properties will not change within the localized
      region. They intend to stay constant. Therefore we must adopt an agent-model to explain
      localized constant properties such as a frame of reference before we can describe a boundary
      within the substrate.

1. Therefore, to even start communicating about boundaries within a substrate we must infer agency
   and intention in order to appropriately slice the substrate into what is relevant from what is
   possible. This is a recursive definition but not infinitely so, as we are grounded in our
   subjective epistemology: the concepts we employ are anchored to our experience of subjectivity
   (self vs not-self), agency (action), and intention (function/relation), existence (I think
   therefore I am).


### Properties, Potentials, Energy and Agency


TODO

:   In this section we show how organizations of automata -- local structures that can only sense
    their local environment --- can create flows of energy, information, and/or matter.


1. Boundaries create potentials by limiting free movement. As potentials build, force builds across
   a boundary, which compels the constituent elements of the border to change their relationship
   according to the stress and strain induced from the potential.

   1. What is potential? non-homogeneous properties, non-uniform agential entities
   1. What is force? An autopoietic propagation? The agential entities that embody the properties are
      intending to propagate?

1. Each structure can only withstand a certain force before deforming. Material deformation is
   divided into two types: elastic or plastic deformation. With elastic deformation, once the force
   recedes the boundary reverts to its original shape. With plastic deformation, the constituent
   elements of the prior boundary permanently alter their relationship and potentially their internal
   properties after the force is removed.

1. A boundary can locally sense potentials through elastic deformation. In this scenario, the dynamic
   definition of the boundary region remains valid---the constituent elements of the boundary keep
   their overall relationships---yet the relative relationship properties such as orientation and
   strength of relationship change between the elements.

1. A boundary can act on potentials through elastic deformation. In this scenario, a potential across
   a boundary induces a strain that stretches, or alternatively a stress that squeezes the boundary
   sufficiently to transform its structure sufficiently such that the boundary no longer holds the
   local properties in place which caused the potential to build.

1. Plastic deformation changes a boundary definition. In this scenario, the relationships between the
   constituent elements of the boundary are permanently altered such that any prior dynamic
   definition of the boundary can no longer be assumed to be valid.

1. When the variability within a substrate remains highly prescribed, then the local properties
   within the substrate are limited and predictable. If the variability deviates outside of certain
   bounds, then localized strong potentials and therefore localized plastic deformations become
   increasingly likely. In such a case prior inferences on the substrate-local entities are less
   likely to hold.

   1. Boundary agents give us a way of mapping reality into function, but such a functional
      representation is only valid within environmental constraints related to the dynamics and
      constituent entities within our description of the environment.
   1. Some of those dynamics may be observable to us and we may know their effects, both elastic and
      plastic, on the local properties of our substrate but there always may be unobserved
      properties and these too may influence the validity of the substrate as our frame of
      reference.
   1. For the observable properties, our information substrate representation becomes suspect based
      on thresholds and relationships we can measure within the representation itself.


### Substrate and Symbolism

TODO

:   In this section we note how we can safely transform the representation of an autopoietic entity
    into a substrate representation with a less natural, more normative set of localized properties
    (symbols) determined by labeling the relevance realizing functions of the autopoietic entity's
    underlying automata.

    We show how automata create and manipulate symbols. Symbols enable emergence and information
    compression, which provides computational complexity benefits for realizing relevance.

    We briefly theorize how qualia may simply be the 'chatroom' of the underlying automata's
    symbolism

1. We define automata to be the constituent elements of an information substrate
   representation. These are the sub-elements and their properties that best describe the
   macroscopic behavior within the substrate so long as substrate-specific dynamical criteria are
   within bounds. Automata distill the underlying substrate into an emergent set of dynamics that
   characterizes that substrate. Through substrates and automata and their relationships with
   respect to potentials and deformation, we turn the natural world into a symbolic representation
   that encompasses fundamental qualities of energy, information, and structure.

1. A substrate itself therefore lives within two orders: the relationships from its constituent
   atomic entities, and then the order of its underlying sub-substrate's relationships.

1. We can describe function, both respect to the transformation and/or dynamics of the fundamental
   qualities of energy, information, and/or structure through the mechanism of agential entities of
   automata. Entities, and inter-entity relationships define structure. Potentials and forces
   describe the dynamics of localized intentional behavior.

1. This recursive definition provides our basis for further defining the normative order and the
   agential model.


### Value Transfer

TODO

:   In this section we explore how to infer a translation function between the dynamics of an
    underlying substrate and the constituent needs of an autopoietic entity as represented in that
    entity's symbolic properties. Æ is autopoietic entity.

- Each action moves energy. Therefore, normative agents must sacrifice energy and other treasures to
  the natural order to act. Indeed their very existence depends on action.
- When an Æ maintains its structure in the face of the continuous insults of the environment's
  agencies --- when it actively counters the wear and tear all material structure is subject to as
  well as the energetic costs of all intentional action--- that implies it has the agential
  potential to stay in autopoiesis. It's budget with the natural order is balanced. How might we
  frame the continuous stream of transactions between agent and environment in order to learn
  something about the nature of normativity, that is the relationship between value and action,
  itself?
- Acquisition of value involves sacrifice of value, as all boundaries exist symbology through
  gradients and edges and all information substrates within which boundaries can exist are (locally)
  zero sum --- something taken must be replaced.
- Rent as an analogy also gives us a means of finding the 'exchange rate' between natural and
  normative value. By careful observation we can map out the energetic cost of an autopoietic
  agent's physical structure. To do this, we will observe and tally the cost to create, maintain,
  and utilize all the systems under the physical control of the normative agent during its
  lifetime. This tells us how much rent is due. Next, we must map each piece of that cost to the
  system's normative order. What value is created by paying this cost? How do individual values
  interrelate to form the system's order? We find these relationships by following the energetic
  transactions, what systems of cause and effect are in place and how do they depend on one another?

  The location of each transjective transaction defines a component of the normative agent's
  boundary with the natural order. The agent's normative order exists inside this boundary, the
  arena exists outside.
- As Æ exist due to boundaries, and such environments are inherently multi-Æ environs, they are in
  constant flux. When an Æ needs a constant symbol/capability (i.e. to maintain cognition, or
  normative order), it must sacrifice to maintain its existence in order to counteract the negative
  normative valenced gradients within its environment.
- Æ define closed boundaries within information substrates in order to create a potential pump. Key
  geometric property is by having a closed boundary potential and information is a property of the
  entire surface while action on the boundary is a point property.
- Agential Capacity comes from the agent's relationship with its environment with respect to action,
  material, and information. There exists a potential gradient for any such object between the agent
  and the environment. Such potentials may even exist when there is no material difference in
  distribution between the two as the _value_ of any quantity is with respect to the autopoietic
  needs of the self, and the agent self and the environment have different needs.
- The fundamental parallel nature of boundary functions enables evolutionary behavior to occur. this
  is the basic generative source of relevance realization opponent processing. If boundary automata
  selectively propagate according to their relative success, then the autopoietic entity can learn
  and adapt. Selective propagation is relative to the set of boundary entities --- the ratio between
  different discriminatory behaviors is what matters, not the absolute number of entities exhibiting
  any particular behavior. This means that both discriminator 'death' as well as discriminator
  replication are how different discriminatory behaviors may propagate.

  In this manner, through the individual actions of each automaton, the collective --- that is the
  shared normative order between the traders --- realizes relevance. The properties of cognitive
  scope (specialized vs generalized behavior), cognitive tempering (explore vs exploit behavior),
  and cognitive prioritization (contextually aware pricing) emerge from the set of boundary
  automata, even if each individual entity is nothing more than a deterministic machine with no
  awareness of the other boundary entities or the collective itself.

### Normative Orders

TODO

:   In this section we generalize our findings about symbolic value transfer to define a
    representation of normative order. Normative orders map the inter-substrate dependencies
    necessary to infer the normative structure of an autopoietic entity. By modeling an Æ's
    normative order we can formally infer the entity's intentions within a particular environment
    state. If we can infer an entity's intentions then we can start to better engineer incentives
    for coordinating pluralistic inter-agent goals.

Likewise, agential intentions must be defined relative to the boundary. The chain of action and
reaction that propagates into and out of the boundary represents a causal information network. As
the network evolves, the information originating from the boundary's exterior mixes and is
transformed by the information state of the interior. To understand intention, we must map this
network and model it. To map this network, we must once again discretize it and thereby introduce
subjectivity onto its phenomenology. Our success at causally modeling the phenomenon depends on our
success at modeling its information network and all the relevant sub-networks therein.

Yet now our network interleaves inputs and outputs such that if we trace action and reaction we are
inevitably left with infinite loops of cause and effect. Yet if we have causal loops within our
model of the system's evolution we are stuck in an infinite regress, for how are we as predictors
supposed to form a prediction when our very model of behavior must calculate indefinitely?


To surmount this obstacle we must define sub-boundaries and sub-intentions onto the information
structure of our original entity. By defining how such subsystems relate to one another we create a
simplified network of causal phenomena, which itself may have cycles but which we can perform the
same transformation to and create new layers of simplified causal phenomena to. By repeating this
process throughout the entity's interior we must finally reach a point where no further
informational consilience is possible and we are left with a set of high level, uncorrelated
entities. There is no reason to presume that we will always be able to distill such a network to a
single entity, so we must presume this high level description is a set of entities, not a single
one.


[Theorem 3]: #theorem-3
[Theorem 3: Normative Order]: #theorem-3
[Theorem 3: Normative Order]{#theorem-3}

:   By inferring intention into the structure of an agential information network we introduce a new
    type of model onto the system that is amenable to discrete calculation. What we introduce is a
    normative order onto the information network. We infer that subsets of the entity's information
    network are stable over time and have stable transformations from input actions to output
    reactions. The stability of such sub-networks is not complete, but sufficiently stable such that
    those sub-networks may be mapped to a simplified description that predicts the sub-network's
    response given an input condition. We label causal networks as having functions.



If we were to map the layers of this functional distillation of the entity's informational network
out, we end up with a lattice network structure, or Directed Acyclic Graph (DAG). The top of the
lattice is our highest level functional description, and the bottom is our highest fidelity
functional description that most closely maps to the forces and transformations we can directly
observe within the agent's environment. We label this lattice structure the agent's *normative
order*.


Can we reliably infer a normative order through observation? Observing the functions of each
element gives us a web of relationships. Each boundary entity acquires symbolic meaning based on its
active relationships as well as the type relationships it can enter. As these relationships and
potential relationships pile on, the complexity of an entity's symbolic nature becomes accordingly
nuanced and unique, similar to how words emerge from an alphabet.

Indeed, different material boundary entity's will acquire different types of relationships which can
only be seen by the role of the sub-networks within the normative system as a whole. Accordingly,
something similar to a grammar emerges, with different entities performing different compositional
functions within systems of greater informational complexity. In this manner, sentences of normative
order emerge from constitutive sub-functions.

Dissimilar from language, where sentences have meaning on their own, these relationships only
acquire meaning if the entire structure of relationship demonstrates the attributes of
autopoiesis. Therefore, we always know the "plot", its just the details of each autopoietic system
that differ. Therefore, if we already now a certain system is autopoietic, and we want to map out
its normative order, we don't need to start from its base boundary entities and work our way up, we
can also decompose its major functions and work our way down to its boundaries. Indeed, due to the
properties of emergence, macro-scale behavior may be even more accurate for elucidating the
underlying structure than building the order up from their base relationships
(@varley2022emergence).

In this way we can translate a physical embodiment into a structured collection of symbols. The
entire structure coexists within the natural arena of its embodiment as well as a bespoke normative
arena of its symbolic, normative order. Each autopoietic self has its own unique context, such that
every normative order will be different. Yet major similarities are bound to exist on multiple
scales of relationship, even among wildly dissimilar systems so long as they exhibit autopoiesis due
to the universal constraints of the property itself.

Can we create a taxonomy of symbolic functions from which we can derive the grammar of normative
order? How about a lexicon? If we can create such references, then our task of inferring our own and
other's normative orders becomes tractable. Let's explore.

What might the major domains, kingdoms and phyla of our normative taxonomy look like? Can we even
decompose relationship into a tree structure in this manner? Not exactly, we have two interrelated
but separate trees at work: a top tree and a bottom tree. When they meet, the idea of autopoiesis
becomes the living fact of autopoiesis. Our top tree is rooted at the idea of autopoiesis; its
kingdoms form the high-level organizational traits that any successful autopoietic entity must
exhibit. Its phylums, classes, etc. are then all the sets of sub-traits from which autopoiesis might
emerge. The bottom tree is rooted in reality, its kingdoms form the large but finite set of
semi-stable structures capable of organizing into rudimentary boundary entities. Its phylums,
classess, etc. form from assemblies and permutations of these constituents into a dizzying breadth
of more complex functional relationships.

Notice that both decompositions quickly break down into nonsense in terms of their potential
size. The sheer number of unique possibilities which could satisfy the root condition is
combinatorial, and thus lose meaning. Yet if we satisfy both conditions, the top-tree and the
bottom-tree, then we have the conditions for life. This type of structure is called a lattice, and
has many interesting mathematical properties. We will revisit lattices and their properties within
our [solution details][]. For now, we just have to see that yes, we can make something resembling a
grammar and a lexicon of normative order. We can create an ever-expanding encyclopedia of known
autopoietic symbols. Each symbol is organized neatly into a lattice structure. Each symbol is
top-rooted to paths by which it plays a role in manifesting autopoiesis. It is then bottom rooted to
one or more paths its functionality physically manifests.

A final note. It may be obvious, but within our lattice structure there is a wide, possibly infinite
number of concepts on the same organizational level as our top and bottom roots. The space of
concepts of the same conceptual order as autopoiesis is unbounded, and the space of physics a the
same conceptual functionality as our boundary is also uncountable. We can discount those
possibilities, focusing on just these two roots as they form the minimum determining factors for
whether a piece of reality has a normative order or not. Indeed, any actualized autopoietic system
will exhibit a large and unique set of concepts from within these spaces and that is part of the
reason why diversity in living systems is interesting.

TODO inflation. marginal utility. balancing the books (flattening emergent complexity orders). These
financial metrics track the interactions relationships between the top and the bottom elements of
the lattice. THe top makes assumptions on the nature of its bottom. These metrics help to track that
relationship and how accurate those assumptions really are. These are ways of representing the
sub-symbolic nature of the autopoietic entity.

- *Inflation*: This is an interesting metric tracking the changing ratio between the internal
  normative value to its denomination in the underlying dependent value (chain reducible to natural
  value) for the autopoietic entity/entities which utilize that order.


Within our framework for normativity, each autopoietic agent acquires a structure of values based on
the unique interplay between the functional relationships of its parts and the selective pressure of
autopoiesis. Values emerge from how physical boundary elements create functional connections with
one another, and how the information within those connections organizes to preserve and propagate
their arrangement. We can identify the boundaries of an autopoietic system by looking for
thermodynamically unlikely quasi-stable arrangements of matter. We can infer a system's normative
order by deriving its constituents of autopoiesis --- thereby inferring its order from the top down,
or by arranging its base functions into increasingly complex modes of functionality --- thereby
predicting its order from the bottom up. We can organize this knowledge into a lattice of
autopoiesis. This lattice structure enables us to classify the types of arrangements we discover in
the world and acts as a shared repository of the language of life itself.

With these capabilities in hand we have almost all the tools necessary to tackle our objective and
increase our capacity for living with meaning. The only high-level concept we are missing is a
theory of how meaning and value are related to one another. If we can overcome that obstacle, then
we can propose a way of organizing ourselves, our culture, and our technology to align our action to
imbuing ourselves and the universe at large with the most meaning.


## A General Grammar of Agency

TODO

:   In this section we gather the concepts from the prior sections in order to present a procedure
    for dynamically calculating the achieved value within a pluralistic community of modeled
    agencies.

    This lets us infer:

    - what is likely relevant for any particular Æ given the state of its substrate
    - how to engineer an Æ that can change the incentives for a pluralistic community of Æs

---

- Concepts

  - Liquid substrate: A sparse graph representation of a substrate.
  - Manifold substrate: A dense graph representation of a substrate. Contains primary connectivity
    as well as a hypergraph of arbitrary types of informational/energetic connections and subgraphs
    along the manifold.

  - substrate.valid: A means of defining when the substrate is no longer autopoietic and therefore
    it's symbolism is no longer valid.
  - substrate.sample(entity_id): a means to acquire the local properties and neighbor entities for
    an entity at the current timestep
  - substrate.connect(entity_id, entity_id)

  - Virtual Particle: A way of representing information and energy that only lasts between timestep
    t (where it's written) and timestep t+1 (where its read).

  - Automata

    - Have two dimensions of connection points:

      - Polar, which has an inner boundary and outer boundary connection point. These points can be
        used to define energy, information, or material transformations between any combination of
        B_in->B_in, B_in->B_out, B_out->B_in, or B_out->B_out

      - Planar, which can contain an arbitrary number of connections to other automata in order to
        define the structure of a manifold substrate. Only information or energy can transfer across
        these connections.

    - Intention is defined by the transformations and virtual particle emissions of each automata
      definiton

FYI, Most of this needs radical rewriting:

[Agential Model]: #agential-model
[Agential Model]{#agential-model}

:   Any autopoietic entity can be modeled as follows:

    - **Environment Reference**: Lays out the assumed properties of the information substrate
    - **Boundary Condition**: separates a closed group within the environment into localized
      [relevance automata][].
    - **Salience Landscape**: mapping between of environment interior into a [salience landscape][]. [Relevance
      Automata][] can sample any aggregate properties within this interior.
    - **Normative Order**: mapping of boundary automata into an execution graph


We believe any such system composed of a mutual information substrate and automata which mutually
interact upon the substrate can be described as a [Salience Landscape][] as defined below:

[Salience Landscape]: #salience-landscape
[Salient]: #salience-landscape
[Salience Landscape]{#salience-landscape}

:   A Salience Landscape is a dynamic undirected weighted network of features consisting of
    relevance-realized features and their normative relationships. A salience landscape minimally
    contains an origin vertex, which denotes the location of the autopoietic self within the
    landscape. The landscape, both in terms of features and of weights, is continuously updated as
    the system [realizes relevance][]. The network weights represent the perceived normative
    relationship between relevant features.

    Within the salience landscape, that which is salient consists of the features and relationships
    that connect the self to other features within the network according to a traversal
    heuristic. Different [opponent processes][] which 'inhabit' the landscape will therefore have differing
    perspectives on what is and what is not salient.

    If such processes reference a shared self-feature, then such processes can mutually realize
    relevance for the self when they each have the power to dynamically update the network through
    their traversal.



## Cognitive Interactions

TODO

:   In this section we identify how cognition is modeled within the language of agency. We identify
    interesting features of inter-subjective communication having to do with the normative
    translation from subjective to objective back to subjective.

    We coin the term normative resonance to define what meaning is and how it comes to be

### Inference and the Imaginal


Intentional action, including communication, is a process of reflection. The agent reflects the arena
internally, recreating the inferred order of the action-environment within the cognitive self. This
has a curious reflective property, the represented actor at the normative bottom of the represented
environment, when in the subjective reference such an anchor is the normative top. The referent
becomes the reference.

When communicating between two subjectivities, this process repeats:

Embedded -> Imaginal -> action -> sense -> imaginal -> Embedded

Without the imaginal realm to translate sense and action, phenomena cannot map to ontology or
perspective. To communicate, both parties must each hold at least two perspectives (self and other),
and two ontologies (self and shared).

The reflective property is due to the difference in perspective. What is 'top' and most close to my
embedded self is what is bottom to my communication partner. In between us is our communication
channel and mediums (who is also a communication partner). When acting/communicating, I must use the
imaginal perspective in order to map my internal context to my partner. This representation has self
as the nesting starting point, and traverses down through my ontology until I gather all the
relevant information to bind into my action, map through my embodiment, into our communication
channel, through my partner's embodiment, to be reversed within their imaginal capacity and thereby
updated into their cognitive self.

### Subjectivity is the Inescapable Reference and Referent of Communication

Our terminology inherently falls within a spectrum of subjectivity. This spectrum spans from
completely subjective to completely objective. Yet no communicable concept is completely one or the
other, rather each term inhabits a mix of the two. Terms map phenomena from the raw stuff of
existence into representation within each of our cognitive landscapes. Some terms, such as
intention, perception, and even agency are much closer to the subjective end of this spectrum than
the objective, yet all terms are flavored by subjectivity, for they all map reality into
representation.

Terms that reside close to the subjective end of this spectrum refer to higher order phenomena
within our cognitive processes. When we use such terms to refer to our *own* behavior versus when we
use them to refer to an *inferred agent*'s behavior we are using them quite differently. All
non-self agency cannot be known objectively. It must be inferred based on our own experience of
subjectivity. When we use subjective terms to refer to our own behavior we know how much meaning the
terms intrinsically hold, for they resonate with our embodied experience. When we use these terms
for entities external to ourselves, we are implicitly inferring that the terms have external meaning
based on relating that external agency to our own embodied ontology. We cannot ever know if they
actually do resonate in a similar manner within the external inferred entity. For, even if we could
rigorously self-simulate the workings of the inferred agent's cognitive apparatus we could not
identify any phenomenology within that simulation as pure representation of the subjective term *as
we know it*. Concept communication is inherently lossy with respect to shared meaning. Subjective
concepts are more susceptible to meaning-loss in their communication as their subjective distance is
greater. All concepts are known or not known from a subjective reference. To communicate a concept
is to 'point' to the concept to another (inferred) subjectivity.

Nevertheless, subjective terms have meaning, for they point to phenomena that are more stable than
the environment at large. With the introduction of subjectivity, we start to discretize the infinite
variability of existence into finite and computable representations. We start to infer emergent
phenomena. The usefulness of such subjective terms is a function of how effective they are at
accurately informing us of the evolution of the environment at the fidelity that we are interested
in compared to the alternative.

What is the alternative? The only alternative is full objectivity, and if we want to predict an
environment's evolution, that is not an option. We can demonstrate this quite easily. For, to define
an environment requires at minimum a single boundary defined. We have two limits to such a
condition: either an infinite or an infinitesimal boundary. In either case we are left with either
an interior or exterior condition to the boundary, and information which may propagate on the
boundary. In either case, if we are to predict the system's evolution we must infer its means of
information flow and transformation, and we must be able to make that prediction external to the
system itself, so we must be able to transfer information about the system into an alternative
representation that we ourselves can process. In order to transfer such information it must be
discretized, or else we presume we can transfer infinite information within a finite span. Even an
infinitesimal point is embedded in the universe, and therefore surrounded by an infinitely divisible
boundary. Therefore in any case where we want to usefully predict a systems' evolution we must
prescribe both a boundary condition and a discretized model of the cognitive processes internal
and/or on the boundary. Any discretization of the information will be flavored by our own ontology
and therefore our own subjectivity. Therefore, any finite message that is useful for prediction will
always be flavored by our own subjectivity.

### Normative Resonance

TODO

:   Meaning occurs when the intrinsic symbolism across AEs create a mutual value gain. Meaning is
    found through the vibration of interaction across the
    imaginal<->subjective<->objective<->subjective<->imaginal medium.

### Shared Hallucinations and the Chinese Room

TODO

:   In this section we identify how parasitic agency occurs. This enables us to show how it changes
    the incentive structures of the executing entities. Such changes can be a net benefit or net
    loss depending on the organization of the parasitic entity.

    We define parasitic agency as entity's that rely on the voluntary or coerced action of normative
    agents as their fundamental substrate such as businesses and governments.

Constructions of authority exhibit autopoietic behavior but do not exist outside of the
cultural artifacts and persons who believe in their reality. They are normatively embodied
beings. They only exist through the shared hallucination of a group of naturally embodied entities
capable of imaginal simulation.

The boundaries of such entities are difficult if not impossible to infer from the natural order,
instead, their normative order and boundaries must be inferred through the actions of the subjective
entities who embody these entities.

Their action is never obligatory in the same manner as embodied obligations. Embodied obligations do
not need to pass through the imaginal to have meaning, but for a subjective entity to act or respond
to an imaginal entity requires them to infer such an action will have normative consequences with
its relations to naturally embodied beings.

Such beings act as normative and ontological shortcuts between entities. The naturally embodied
beings do not need to go through the proof of the normative benefits of acting a certain way to a
certain entity so long as they can fit themselves and their action-partner into the ontology of the
normatively-embodied entity.

## Conclusion

TODO

:   In this subsection we summarize by pointing out:

    - interesting features of our proposed language,
    - contrasting it with other metaphysics and/or analytic approaches and
    - interesting applications of the language.
    - roadmap of further work

We find the concept of a salience landscape compelling for the following reasons:

1. Its potential complexity is boundless depending on the instantiated automata, the properties of
   the edge relationships, and overall number of features. A salience landscape may be static sparse
   and static (with respect to the number of automata and/or features), or dense and dynamic. The
   structure can therefore describe a myriad of autopoietic entities ranging from the simple to the
   complex.
1. The approach is composable, such that an automata within one landscape may itself be described by
   its own salience landscape.
1. The structure is dynamic and rich: for example the landscape itself can feed back into the
   heuristics executed by the inhabiting automata.
1. The landscape is composable in that salience is related to the network connectivity, which is a
   combinatorial property, not the absolute number of feature vertices within the landscape.

Although we define this structure in algorithmic terms, we believe it maps to a myriad of physical
phenomena, notably any structure where local automata have localized sensory access to aggregate
properties.

We believe that all phenomenological phenomena can be mapped onto a relationship between an automata
and a salience landscape. As an example, we believe this description maps to the functionality of a
living cell, which we can describe as follows: A basic cell is composed of a closed membrane
consisting of a set of (literally) cellular automata constituent of the membrane as well as existing
within the membrane. These automata exist as localized sources and sinks which translate the
aggregate state of the cell into localized actions by responding to the physical properties of their
local environment. The membrane and its interior can be said to realize relevance when the
automata---that is the material sub-components constituent of the closed membrane and its
interior---together react to preserve and propagate the membrane.

Additionally, a network of neurons also exhibits this structure. If we take each neuron to be its
own automata then the input message accounts for the neurons internal state, the attention traversal
accounts for the neurons received action potentials, the intention traversal accounts for the
neuron's (lack of) firing, and the output message accounts for the neuron's updated internal state.

What then does this provide us in pursuit of our objective? Primarily it provides us with a
terminology that we will use to explore the nature of self, agency, meaning, and value all within a
context that we may readily translate into the states and automata within a salience landscape,
while avoiding the [homuncular fallacy][] and the [frame problem][]. The remainder of this document
explores each of these subjective phenomena in turn.
