# PIBBSS Affiliate Program Application

Applicant Information
:   \

    Name
    :    Andrew Lyjak

    Email
    :    andrew.lyjak@gmail.com

    Cell Phone
    :    1+(734) 604-6163

    Web
    :    [substack](https://buildonomy.substack.com/), [twitter/X](https://twitter.com/spacelyjak), [linkedIn](https://www.linkedin.com/in/andrew-lyjak/)

    Address
    :    46800 Betty Hill, Plymouth, MI 48170

Dear PIBBSS application committee,

What follows is a narrative on why I'm applying to your Research Affiliate program (788 words), and
my current research roadmap (689 words). These sections compose the answers to the personal
statement prompt. I conclude with a summary of my prior engagement with AI risks/AI Alignment (285
words).

## About my Research

From 2010-2018 I led the Flight Software safety and quality design process for SpaceX. While there I
began investigating how to effectively navigate group coordination problems. I sought to preserve or
enhance individual agency while honoring system constraints. Given modern software tools, it seems
better solutions are possible than currently exist. As the Software Safety and Quality expert, I
worked to remove myself from product safety and quality workflows. I created protocols and
interfaces by which our products were designed and built. My objective was to show, using
engineering, production, and verification processes, that each type of software quality and safety
coordination problem was rigorously solved by the discipline experts without inserting myself
directly into the workflow.

After SpaceX, from 2018-2023, I was employed as a software release engineer at Raytheon, then [H3D,
Inc](https://h3dgamma.com/). In these roles, I designed and built software infrastructure to act as
an automated factory for building and testing my employer's application software. Here I built tools
whose sole job was to make those aforementioned coordination problems easier to navigate.

In addition, I have acted as an unofficial collaborator for my wife, Dr. Brandalyn Riedel, in her
Neuroscience PhD work. I supported her development of large human brain imaging machine learning
pipelines. This gave me skills for understanding and building machine learning software. Although I
have not had the opportunity to train deep learning models in my career, I have completed two of
Andrew Ng's deeplearning.ai coursera courses (see certifications:
[1](https://www.coursera.org/account/accomplishments/verify/26WQCWABQKDV),
[2](https://www.coursera.org/account/accomplishments/verify/6XH7UGXEJDQZ)).

Last year (2022), my wife and I were finalists in the [Future of Life
Institute](https://futureoflife.org/)'s [worldbuilding
contest](https://worldbuild.ai/W-0000000101/). The objective of this contest was imagining a
realistic and optimistic vision for the world in 2045. One of the constraints was that AGI was
achieved and played an important role in how the world works.

For this exercise I researched the latest arguments about AI Alignment, read
[Superintelligence](https://www.goodreads.com/en/book/show/20527133) and many essays on
[alignmentforum.org](https://www.alignmentforum.org/), and attended a Responsible AI Symposium. The
primary concern related to AGI Alignment is that since an AGI would not be constrained by biological
or cultural limits, and engenders the possibility of bootstrapping itself to god-like levels of
intelligence, we have no reason to trust that such an intelligence would value human (or planetary)
well being.

I believe the premises behind this stereotypical alignment argument is incomplete. In broad strokes,
the argument is too cavalier with terms such as intelligence, value, and agency. These terms cannot
be presumed to be universal properties. Instead they are extremely context-sensitive. They derive
from the relationship between a cognitive entity and its embedding environment. This has
repercussions for foundational premises on which a successful approach to alignment must be based.

We cannot constructively address the problem of AI Alignment until we enhance our cultural
technology related to the comprehension-of and communication-of values. Once we can communicate
values legibly, what we are likely to find is that alignment becomes a problem of introducing a new
stage of [ecological succession](https://en.wikipedia.org/wiki/Ecological_succession) into the
global cultural landscape. We are introducing new 'species' into our culture, and must ensure our
cultural trophic levels reach a suitable dynamic attractor. In other words, the 'invasive species'
of AI must become native and integrated, so as to not overwhelm our biological and cultural modes of
existence.

My **motivation** for applying to the PIBBSS Affiliate program is to structure and refine my
argument to enable the design and instantiation of a protocol for better expressing intentional
action. Similar to my work at SpaceX, the purpose of this is to insert a protocol within
pre-existing contexts in a manner that helps the active parties achieve their purpose, while also
providing a detailed transaction log to be analyzed post-fact to understand the nature of the
interaction and thus learn and correct behavior at the individual and collective level.

My **research interests** relate to all areas necessary to actualize a successful protocol along
these lines. I then plan to integrate these results into an organization I am developing, named
Buildonomy. The purpose of Buildonomy is to develop and produce "agency' prosthetics. Roughly,
Buildonomy's product(s) exist in the space of [Product Lifecycle
Management](https://en.wikipedia.org/wiki/Product_lifecycle) and help users navigate the web of
intentions they participate within, both individually and pluralistically (i.e. as corporations and
coalitions). The aim of Buildonomy is to provide individual and collective benefit through
dissemination of a set of protocols and tools for coordinating, defining, and refining intentional
action. The value proposition is that individuals and collectives would have a better mechanism for
sense-making in a pluralistic world through optimized syntax for describing values and intentions
within and among themselves, and that refined syntax would enable safe(r) integration of automation
(including AI technologies) within the [markov
blanket](https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0792)s of individual and
pluralistic agents.

## Research Roadmap

My existing and planned work is organized into the following research questions. Projects I
**intend** to pursue as a research affiliate are tagged [PIBBSS]. Links to my completed work are
tagged [CW].

1. **Define a protocol suitable for reliably inferring intentions**
   1. [In this post](https://buildonomy.substack.com/p/the-importance-of-inferring-intention) [CW] I
      lay out my argument on why inferring intention is necessary for social stability and outline
      constraints for a project to improve our innate intention inference capacities.
   1. Intentions do not need to be similar to be aligned. I outline a specific example of how this
      plays out [in this essay](https://buildonomy.substack.com/p/surfing-with-the-maelstrom) [CW].
   1. I am currently studying [Active Inference: The Free Energy Principle in Mind, Brain, and
       Behavior](https://mitpress.mit.edu/9780262045353/active-inference/) as part of the fifth
       cohort of the [Active Inference Institute's textbook
       group](https://www.activeinference.org/education/textbook-group). I intend to then study
       [Bayesian Mechanics](https://royalsocietypublishing.org/doi/epdf/10.1098/rsfs.2022.0029) in
       more detail, which I believe can serve as the theoretical basis for analytically defining
       intentions and other cognitive phenomena within a measurable state space. Specifically, the
       Constrained Maximum Entropy Principle and its Gauge symmetry to the Free Energy Principle
       affords a mathematical language for translating between objective and subjective perspectives
       on intention and identity. This will be completed [CW] prior to potentially entering the
       affiliate program.
   1. [PIBBSS] Explicate the relationship between Bayesian Mechanics and the nature of identity and
      value. Identify the lessons the framework provides on the nature and solutions to alignment.
   1. [PIBBSS] Explicate how procedures serve as an expressive method for coordinating intra-agent
      intentions. Unlike code, procedures imply the executing body maintains a degree of
      responsibility and autonomy for their actions. Exploring the second order effects of this
      autonomy reveals promising directions for how to communicate and coordinate inter-agential
      value and intentions.
   1. [PIBBSS] Rewrite [The Importance of Inferring
       Intention](https://buildonomy.substack.com/p/the-importance-of-inferring-intention) to
       incorporate review feedback and replace the proposed approach with references to Bayesian
       Mechanics Identity-Value results outlined above.
   1. Collective impacts of supply chain legibility was included as a component within my
      [worldbuilding contest entry](https://worldbuild.ai/W-0000000101/) [CW]. I propose that if
      consumers were better equipped to understand the supply-chains behind their products it would
      create more diversity and robustness within the global supply chain. Were we able to
      efficiently provide consumers with an accurate summary of what entities their purchases help
      and hinder, market dynamics would likely fractionate into a diverse constellation of values. I
      intend to pursue this functionality as the proof of concept application for Buildonomy. This
      theory has implications for AGI alignment as it demonstrates forces that pull a system towards
      distributed stability points rather than consolidated mega-structures; which is a principle
      assumption behind many 'instrumental power' arguments of AGI ruin. If the theory is correct,
      it may inform mechanism design that would act to prevent instrumental convergence into AGI
      mega-structures, thereby enabling a dynamical ecosystem of diverse human and AGI derived
      normative orders to flourish.
      1. [PIBBSS] Generate an essay laying out this theory including testable hypotheses.
      1. [PIBBSS] Generate a proof of concept consumer-accessible supply-chain legibility
         application. *This activity is estimated to take at least three months. The other work in
         the research program will constrain and inform the application's design.*
1. **The nature of agential substrates** Agents are embedded in one or more landscapes with
   different communication and coordination properties, which I label agential substrates. The
   nature of their markov blanket is different within each substrate, although a single identity may
   participate within multiple such substrates.

   1. The properties of each substrate define what is and is not relevant to an entity, which has
      implications for learning. My essay [Sources of
      Relevance](https://buildonomy.substack.com/p/sources-of-relevance) started to explore a facet
      of this [CW].
   1. [PIBBSS] Generate an essay on how human collectives are intrinsically different from other
      animal collectives. The fact that we generate our own cultural substrate and then participate
      as group members within multiple cultural groups is a zero to one change versus the animal
      kingdom. Within the animal kingdom there is a clear one to many hierarchy in terms of emergent
      organization. For example, ants only participate in a single colony, whereas a human may
      participate within multiple unrelated hierarchies simultaneously. On this axis, our behavior
      is more akin to an operating system that is executing multiple 'cultural applications'
      simultaneously, rather than anything found in the natural kingdom.

## Extent and Nature of my prior engagement with AI Risks/AI alignment

- As part of research for Future of Life Institute's worldbuilding contest (entry #101) I engaged
  with contemporary thought on AI risks and AI alignment. Many of my thoughts on the manner are
  captured in my entry. A summary of material that I studied includes:

- Superintelligence by Nick Bostrom
- [2022 Responsible AI Symposium by AILA](https://www.joinai.la/events/responsible-ai-symposium-2022)
- Most of the content within https://arbital.com/explore/ai_alignment/
- Numerous essays on alignmentforum.com and lesswrong.com, my favorites being:

    - The [Embedded Agency Sequence](https://www.alignmentforum.org/s/Rm6oQRJJmhGCcLvxh) by Abram
      Demski and Scott Garrabrant
    - [Multiagent Models of Mind](https://www.alignmentforum.org/s/ZbmRyDN8TCpBTZSip) by Kaj Sotala
    - [AGI Safety from First Principles](https://www.alignmentforum.org/s/mzgtmmTKKn5MuCzFJ) by
      Richard Ngo

- I drafted a piece for lesswrong/alignmentforum entitled "Alignment and Freedom" that attempts to
  show how any single AGI, regardless of its "alignment", is likely to artificially restrict freedom
  contrary to the ideals of Liberal Political Philosophy. It isn't published because of the squishy
  nature of terms like agent and value in the discourse. The solution is to ensure there's a
  diversity of AGI-enhanced entities in order to ensure there's a dynamic interplay of value systems
  within the cultural ecosystem. Getting the essay published is on my near term radar.

- Many of my essays on buildonomy.substack.com are circling aspects of the problem of AI alignment
  through the lens of looking at problems related to human coordination.

- Much of my work on engineering quality and safety processes for human-flight-certified flight
  software development is applicable to issues surrounding AI alignment, where the intention to
  advance the state of the art must also be balanced by the risk of the effort, and the properties
  that must be measured are a mix of squishy human intentions as well as hard constraints related to
  human biology and physics.
