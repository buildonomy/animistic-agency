# Proposal: A Feedback and Accountability Process for Certification Working Groups

Certification is an inherently adversarial process. Product owners endeavor to design and construct
a system capable of surpassing product success criteria whereas V&V criteria owners ensure the
intention of such criteria are satisfactorily met. Although this relationship is adversarial,
product owners and criteria owners are still partners; they simply approach the relationship between
product and evaluation criteria from different perspectives. As such, each certification working
group represents an adversarial partnership. Participation is aligned when both partners are united
by the overarching goal of ensuring program and operational success.

A well-run certification program is a positive sum proposition for the underlying program. The
conflict between criteria and product improves both the product, the criteria, and program
itself. Nevertheless, the structure of any adversarial partnership, introduces opportunity for
misalignment between the incentives product owners, criteria owners, and program itself. As program
managers are the most likely parties to be aligned mission success --- they are after all
accountable for managing the program in order to achieve its operational objectives --- they are
least likely to experience misalignment between their role and their personal incentives. As such,
misalignment within the certification process can be adequately controlled through the active
participation of program management in assessing the performance of each certification team. This in
itself presents considerable difficulty as the reason to have separate teams in the first place is
due to the inherent complexity and domain expertise necessary to evaluate the subsystems of any
complex aerospace program. Therefore, mission management must principally operate against aggregate
and standardized information in order to assess whether more direct intervention is necessary to
resolve performance issues within a certification team.

To address the issue of certification misalignment, Astrolab proposes incorporation of the following
process for collecting and assessing metrics on the performance of all NASA-Astrolab certification
partnerships. This process is designed to fit easily into existing program workflows and helps to
ensure that certification responsible owners (product, certification criteria, as well as mission
management) stay accountable to the goals and constraints of the overarching Lunar Vehicle
Program. The proposal is as follows:

## Collection of Standardized Performance Metrics:

This proposal revolves around collecting and assessing metrics on the perceived value and effort
expended by product and criteria owners while performing their responsibilities with one another. By
aggregating metrics in a standard manner across each data product required for certification
purposes, the performance of each insight and certification working group team can be fairly
assessed and structural or personnel issues quickly brought to light in a fair and
non-discriminatory manner. Astrlab proposes that criteria and product teams each independently
annotate each certification collaboration item they bear responsibility for with the following information:

1. The value the item represents to the program. Proposed value ratings of:

  * -3: Subtracts significant value from the program
  * -2: Subtracts value from the program
  * -1: Subtracts insignificant value from the program
  *  0: Neither adds nor subtracts value away from the program
  * +1: Adds insignificant value to the program
  * +2: Adds value away to the program
  * +3: Adds significant value to the program

2. The number of hours the team spent in aggregate authoring, reviewing, revising, and/or responding
   to the item

We define certification collaboration items to encompass the following data products:

- DRD Deliverable
- Milestone Data Deliverable
- RID
- Working Group Agenda Item

In addition, for each working group, we propose that the following information be freely
disseminated across all stakeholders in the partnership:

For each working group, the following information is kept up to date and accessible to all partners
in the program:

* Each working group standing member's name, contact information, and responsibilities. At minimum,
  each working group will have a separate NASA and Astrolab point of contact who are jointly
  responsible for reaching consensus on the agenda for each working group.
* For each working group meeting the following is recorded: attendance, and the agenda items that
  were covered in the meeting.


## Tracking and Correcting Misalignment

Through aggregation and review of the data defined above, the program can fairly assess and track
each working groups' alignment to the objectives and values of the program itself. Below we propose
metrics built upon this data helpful in identifying the different failure modes of adversarial
partnerships. In general, these failure modes fall into two categories: perverse incentives; where
the incentives of one or more teams do not align with program success, and problems of insufficient
expertise for the domain knowledge of the underlying content. We identify sub-causes for these
failures and methods to detect them from the proposed data.

Below we provide descriptions and means of detection of failure modes inherent to the process of
adversarial partnership. As with all inter-personal relationships each specific case must be handled
on its own merits. The entire program-criteria-product partnership must be aware of the pitfalls and
solutions to these issues for the highest probability of success. Program management bears the
largest responsibility, for they must provide guidance and oversight to all domain-specific working
groups the program relies upon for its success; as such they bear most responsibility for wisely
interpreting this data. Indeed, they are also most responsible for identifying issues and
recommending corrective action. The tools available for successful corrective action are numerous
and we do not attempt to enumerate them exhaustively. Some examples of types of corrective action
include personnel retraining, reassignment, process changes, and even re-alignment of the
fundamental constraints of the program, product, and/or evaluation criteria. In this proposal we do
not attempt to proceduralize this process, but only point to how issues might be identified by
interpreting the proposed data.


### Cause: Perverse Incentives

Perverse incentives are when team members are inadvertently rewarded for behavior that is
net-negative for the program-at-large.

#### Subcause: Misalignment Between Product/Criteria and Mission Success

Description
:   Product and criteria owners have a tendency to favor their expertise even when that expertise is
    not a good fit for the program. Generally this is due to sunk cost fallacy: product and criteria
    owners have invested time and effort into their area of expertise, and so may remain blind or
    oblivious to alternative solutions. This will result in attempts to shoe-horn their solution
    into the program, resulting in invalid or sub-optimal mapping between success criteria to
    product and/or program.

Detection
:   * **Negative value comments**: Issuing a negative value assessment against any certification
      coordination item is a red flag that deserves mission management oversight. Negative value
      comments may be a response to one party attempting to apply their chosen solution within a
      domain within the program where it does not apply.
    * **Lack of coordination training**: When a working group team is not sufficiently versed in the
      fundamental constraints and assumptions on the other sides of the program triangle (triangle
      being the relationship between criteria, product, program), then they are more likely to apply
      their expertise to a domain where it does not belong. By tracking the hours logged in
      reviewing products from the two other sides and comparing to the hours logged on other
      activities it may be possible to elucidate when the team is disregarding the program
      constraints in favor of the 'purity' of their chosen domain. For example a criteria owner in a
      certain domain's education on the fundamentals of the product and program they are supposed to
      apply their criteria to.


#### Subcause: Regulatory Capture

Description
:   A working group loses its value to the program if the group strays too far to the partnership
    end of adversarial partnership. The purpose of separating a working group into teams of
    adversaries -- one responsible for aligning the product to the program and one responsible for
    aligning the criteria to the program -- is to ensure the work performed for the program is
    adequately challenged and de-risked prior to initiating real-world operations. This structure
    ensures that the group in-aggregate stays responsible for synthesizing criteria to product
    within the context of the overarching program. If the product or criteria teams become too
    flexible to the desires of the other, then the essential challenge of the environment breaks
    down. This may result in overconfidence on the applicability and robustness of the solution to
    the program's needs. Such a breakdown can occur when the team member's see personal value in
    overly accommodating the other. This may be due to many factors, not least of which their job is
    materially easier if the inherent challenge is reduced.

Detection
:   * **Lack of challenging feedback cycles**: If feedback cycles on certification criteria items
      does not take much time to resolve, it either means the chosen solution is optimal, or the
      working group team is not performing well. This may be read through the ratio of time spent on
      initial product generation versus work spent on generating and responding to feedback.
    * **Auditing/IVV of Working Group Work Products**: As both teams may have an implicit agreement,
      they may be able to doctor the books such that aggregate data is meaningless. As such,
      independent auditing of their material and work products by a third party (mission-management
      or an external contracted entity) may help to identify negligence due to regulatory capture.

### Cause: Insufficient Expertise

Aerospace programs are inherently multi-disciplinary and state of the art, so they inherently have
issues due to misapplication and lack of expertise in their core team members. The following
subcauses identify different types of failure mode due to insufficient expertise within working
group sub-domains critical to program success.

#### Subcause: Responsibilities Exceed Capabilities

Description
:   Finding, recruiting, retaining, and training sufficient expertise to ensure their success is a
    challenging endeavor. Due to the field's allure, personnel may be incentivized to disguise their
    capabilities, advertising themselves as offering more than they can be reasonably trusted to
    provide. The intrinsic dialogue embedded in the structure of a working group helps to expose
    such deficiencies, enabling the program to quickly course correct when risks due to insufficient
    expertise are identified.

Detection
:   * **Negative and Low Value Feedback**: Issuing a negative value assessment against any
      certification coordination item is a red flag that deserves mission management oversight. When
      a particular team member accrues a particularly low aggregate value against the material they
      generate (in terms of initial coordination items or feedback items), it is likely their
      responsibilities exceed their capabilities.
    * **Lack of Feedback**: When team members stay silent and do not generate much initial or
      feedback coordination items it may mean they lack the self-assurance to make their voice
      heard. This may be due to structural issues or it may be because their responsibilities exceed
      their capabilities.


#### Subcause: Low Technological Readiness Level (TRL)

Description
:   Certain products and mission programs require solutions that do not have significant exposure to
    real-world environments. In such a scenario, there do not exist available experts who can
    reliably navigate the program, product, and or success criteria to assured success. As such, the
    expertise must be developed within the program's early lifecycle. When programs plan for such
    difficulties from the onset for low TRL within particular sub-domains, they can meet the
    challenge through assigning particularly skilled personnel and additional resources to the area,
    but it is also possible for low TRL arease to be identified relatively late in the program. Such
    areas may at first appear as any of the above failure modes, but spread evenly across all teams
    within the working group's domain. Evaluation criteria will not be sufficient to tackle critical
    issues in the product early in its design, product design will have to undergo numerous redesign
    efforts, and feedback and certification coordination items will be extensively revised.

Detection:
:   * **Extensive Review Cycles**: Certification coordination items will be addressed and revised in
      many more cycles than is necessary for other higher TRL areas of the program. As such, the
      ratio time spent generating and reviewing initial products compared to time spent reviewing
      and responding to feedback will be significantly lower -- that is much more review and
      response time compared to higher TRL areas.
    * **High Value Feedback**: In a low TRL domain, the assessed value of initial certification
      coordination items is likely lower than compared to high TRL domains. In contrast, the value
      of review and response items is likely to be higher in low TRL domains than high TRL domains,
      as review is more likely to discover issues liable to significantly alter the working group's
      plans, designs, and/or success criteria.

## Arguments and Counterarguments:

Argument: Self-reported metrics, collected through surveys and the like, present an intrinsic
problem of data quality and consistency. Such metrics must be freely given in order to be of
sufficient quality to use, and most givers of such metrics have no personal incentive to offer them,
as their value is all captured in their aggregation and not the individual records.

Counterargument: In the proposal we can divide the personnel into three competing teams, each of
which is incentivized to provide accurate information in order to make their job easier. For
example, if criteria owners are given inadequate product data for their evaluation purposes, it will
take them longer to assess and the data sources they are provided with represent only marginal value
for their programmatic role. By accurately recording the effort the criteria team puts into review,
endemic issues with the pedagogical value of the underlying data source are exposed separately from
the day-to-day responsibility of identifying material issues (RIDS) in the performance of the
product for acheiving programmatic success. By expressing the value and effort the team perceives
for each deliverable, when inevetable conflicts between their adversarial partners arise, such as
when mission managers think the process is taking too long, or when product owners can't understand
why their results are not quickly accepted forthright, this assessment data protects the team by
adding quantitative rigor to their responsibilities in a way that can be fairly assessed in
comparison to the effort and value perceived by their counterparts.
